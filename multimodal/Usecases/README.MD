```markdown
# FedOps â€¢ Hateful Memes Federated Learning

This repository demonstrates a PyTorchâ€based federated learning (FL) setup using [FedOps](https://github.com/FedOps/FedOps) and [Flower](https://flower.dev/) on the â€œNeuralCatcher/Hateful Memesâ€ multimodal classification dataset. Each client trains a simple BERTâ€‰+â€‰CNN fusion model on local imageâ€‰+â€‰text splits, and a central FL server (FedOps/Flower) aggregates updates via FedAvg. You can easily switch to nonâ€IID data splits, customize aggregation strategies, or plug in your own FL algorithm.

---

## ğŸ” Project Overview

- **Dataset**:  
  [neuralcatcher/hateful_memes](https://huggingface.co/datasets/neuralcatcher/hateful_memes)  
  Combines meme images with overlaid text and binary labels (hateful vs. nonâ€hateful).

- **Model**:  
  - **Text branch**: preâ€trained BERT (pooler output of size 768)  
  - **Image branch**: simple 2-layer CNN â†’ adaptive pooling â†’ fully connected to 128  
  - **Fusion**: concatenate [768â€‰âŠ•â€‰128] â†’ 256-dim fusion â†’ 2â€class classifier  

- **Federated Learning**:  
  - Flowerâ€™s FedAvg strategy (configurable via Hydra).  
  - â€œHoldâ€outâ€ 10% validation on the server side for global evaluation.  
  - Clients each load (IID by default) full training dataâ€”but you can replace with nonâ€IID splits.  
  - A lightweight FastAPI â€œClient Managerâ€ on each node triggers `/start`/`/stop` for local training.

---

## ğŸ“‚ Repository Structure

```

Hateful\_memes\_classification/
â”œâ”€â”€ conf/
â”‚   â””â”€â”€ config.yaml         # Hydra config: hyperparams + FL strategy
â”‚
â”œâ”€â”€ data\_preparation.py     # PyTorch Dataset + DataLoader logic for Hateful Memes
â”œâ”€â”€ models.py               # Fusion model definition + train/test helpers
â”‚
â”œâ”€â”€ server\_main.py          # Hydra â†’ FedOps FLServer entrypoint
â”‚
â”œâ”€â”€ clienta\_main.py         # Hydra â†’ FedOps FLClient entrypoint (actual local training)
â”œâ”€â”€ client\_manager\_main.py  # FastAPI â€œmanagerâ€ that polls server and triggers client
â”‚
â””â”€â”€ requirements.txt        # All Python dependencies

```

- **conf/config.yaml**  
  All shared hyperparameters (batch size, learning rate, number of rounds, model architecture, FL strategy, etc.). Every Python script uses Hydra to load the same config.

- **data_preparation.py**  
  - `HatefulMemesDataset(split, max_length)` â†’ loads Hateful Memes via HuggingFace Datasets, tokenizes text with BERT, downloads + preprocesses image.  
  - `load_partition(batch_size)` â†’ returns `(train_loader, val_loader, test_loader)` for a single client.  
  - `gl_model_torch_validation(batch_size)` â†’ serverâ€™s 10% holdâ€out validation loader (no client sees this during training).

- **models.py**  
  - `HatefulMemesFusionModel` â†’ multiâ€modal fusion network.  
  - `train_torch()` â†’ returns a function `train(model, loader, epochs, cfg)` that moves everything to GPU/CPU, performs local updates, and returns the updated model.  
  - `test_torch()` â†’ returns a function `test(model, loader, cfg)` that evaluates on validation/test data and returns `(loss, accuracy, metrics_dict)`.

- **server_main.py**  
  - Reads `conf/config.yaml` via Hydra.  
  - Instantiates initial global model (`instantiate(cfg.model)`).  
  - Builds a single global validation loader via `gl_model_torch_validation`.  
  - Wraps `test_torch()` into `test_wrapper(model, loader, cfg)` for serverâ€side evaluation.  
  - Creates and runs `FLServer(cfg, model, model_name, model_type, gl_val_loader, test_wrapper)`, which spins up Flowerâ€™s FL server loop.

- **clienta_main.py**  
  - Reads `conf/config.yaml` via Hydra.  
  - Selects GPU if available, prints device.  
  - Sets random seeds.  
  - Loads local `(train_loader, val_loader, test_loader) = load_partition(cfg.batch_size)`.  
  - Instantiates fusion model â†’ `.to(device)`.  
  - Downloads any local checkpoint if exists (`client_utils.download_local_model`).  
  - Prepares `train_torch()` and `test_torch()`.  
  - Registers everything in `registration = {train_loader, val_loader, test_loader, model, model_name, train_torch, test_torch}`.  
  - Launches `fl_client = FLClientTask(cfg, registration)` and calls `fl_client.start()` to connect to Flower server.

- **client_manager_main.py**  
  - A FastAPI app that periodically:  
    1. Polls serverâ€™s â€œ/infoâ€ endpoint (via `requests.get(http://<serverST>/FLSe/info/<task_id>/<mac>)`).  
    2. Once server signals â€œreadyâ€ for the next round, calls `POST http://localhost:8003/start {server_ip, client_mac}` on the local client.  
    3. Implements `/trainFin` and `/trainFail` endpoints for the client to report status.  
  - Ensures each client does not begin local training until the FL server authorizes it.

- **requirements.txt**  
```

fedops
torch
torchvision
scikit-learn
pandas
numpy
tqdm

transformers
datasets

Pillow
requests

uvicorn
fastapi
pydantic
hydra-core
omegaconf

````
Install with:
```bash
pip install -r requirements.txt
````

---

## âš™ï¸ Installation / Setup

1. **Clone this repository**

   ```bash
   git clone https://github.com/<your-org>/Hateful_memes_classification.git
   cd Hateful_memes_classification
   ```

2. **Create a Python 3.9+ virtual environment**

   ```bash
   python -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies**

   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **(Optional) Verify GPU availability**

   ```bash
   python -c "import torch; print(torch.cuda.is_available())"
   ```

---

## ğŸš€ Running the FL Server

On a dedicated â€œserverâ€ machine (or Kubernetes pod), run:

```bash
cd Hateful_memes_classification
python server_main.py
```

Hydra will load `conf/config.yaml` and start Flowerâ€™s FL server. You should see logs like:

```
â„¹ï¸  Building global model: HatefulMemesFusionModel
â„¹ï¸  Loading validation split for global evaluation (10% holdâ€out)
â„¹ï¸  Flower server running: â€¦ (port 40025)
â„¹ï¸  [ROUND 0] initial (loss, metrics): 0.26, {â€˜accuracyâ€™: 0.58}
```

By default, Flowerâ€™s gRPC port is `40025` (you can override via environment variables or Hydra if needed).

---

## ğŸš€ Running a Single Client (Standâ€alone)

If you want to test locally with just one client (no manager), you can do:

```bash
cd Hateful_memes_classification
# In one terminal: start server_main.py
python server_main.py

# In a second terminal: start client_main.py (renamed to clienta_main.py here)
python clienta_main.py
```

You will see:

```
ğŸ”Œ Using device: cuda:0
ğŸ”§ Step 1: Setting random seeds
â€¦ (data loading logs)
ğŸ§  Step 4: Instantiating fusion model
ğŸš€ Step 8: Launching FL client taskâ€¦
[flwr][INFO] Connecting to server at ccl.gachon.ac.kr:40025
â€¦
```

That single client will join Round 1 immediately. Since `clients_per_round=2` in `config.yaml`, you might need at least two clients to fully kick off FedAvg. To simulate two local clients on one machine, run `python clienta_main.py` in two separate shells (theyâ€™ll both contact `localhost:40025` by default). Each will load the full dataset (IID) and train locally for `num_epochs=1` before uploading.

---

## ğŸš€ Running with Client Manager (Kubernetes Style)

In production/Kubernetes, each client runs two processes:

1. **Client Manager** (FastAPI, port 8004)

   ```
   python client_manager_main.py
   ```
2. **FL Client** (Hydra + Flower, port 8003)

   ```
   python clienta_main.py
   ```

The manager polls the serverâ€™s `/info` endpoint every few seconds. When Flower server is ready for Round n, the manager calls:

```
POST http://localhost:8003/start
  { "server_ip": "<serverâ€hostname>", "client_mac": "<device_mac_address>" }
```

The client then begins local training, and upon completion hits `/trainFin` or `/trainFail` on the manager. This twoâ€process pattern is useful in Kubernetes so that your container doesnâ€™t block solely on Flowerâ€™s longâ€running local loop.

---

## ğŸ”€ Nonâ€IID Data Splits

By default, each client calls:

```python
train_loader, val_loader, test_loader = load_partition(batch_size=cfg.batch_size)
```

which uses the **entire** Hateful Memes training split. To create a â€œgoldâ€standardâ€ nonâ€IID partition:

1. **Modify** `data_preparation.py` to support splitting `self.dataset` by label or text cluster. For example:

   ```python
   def load_non_iid_partition(client_id: str, batch_size: int):
       full = load_dataset("neuralcatcher/hateful_memes", split="train")
       # e.g. stratify by label: client â€œAâ€ gets all negative-label memes, client â€œBâ€ gets positive-label
       # Or cluster by text features â†’ partition accordingly
       client_subset = full.filter(lambda x: x["label"] == desired_label_for_client(client_id))
       train_data = HatefulMemesDatasetFromHF(client_subset, split="trainâ€pseudo", max_length=128)
       # â€¦ same for validation/test or let server hold them out
       train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, â€¦
       return train_loader
   ```
2. In each clientâ€™s entrypoint (`clienta_main.py` vs. `clientb_main.py`), call:

   ```python
   train_loader = load_non_iid_partition(client_id="A", batch_size=cfg.batch_size)
   ```

   rather than `load_partition(...)`.
3. Keep the serverâ€™s `gl_model_torch_validation(...)` unchanged: it will load 10% holdâ€out from the full split.

This guarantees that no client sees the entire distribution and each has its own skewed subset.

---

## ğŸ“¦ Custom Aggregation Strategy

If you want to implement a novel FL aggregation (e.g. â€œModalityâ€Aware FedAvgâ€), you can:

1. **Create a new Python file** under `fedops/server/strategies/`, for example:

   ```
   fedops/server/strategies/multimodal_aware.py
   ```

   and define your custom strategy class inheriting from `flwr.server.strategy.Strategy`, overriding `aggregate_fit` or other hooks.

2. **Modify** `conf/config.yaml`:

   ```yaml
   server:
     strategy:
       _target_: fedops.server.strategies.multimodal_aware.MyCustomStrategy
       # â€¦ any extra hyperparameters for your strategy
   ```

3. In `server_main.py`, because the configâ€™s `server.strategy._target_` now points to your custom class, FedOps will automatically instantiate it instead of `FedAvg`.

Later, you can package `fedops/server/strategies/multimodal_aware.py` (and any necessary helper modules) into a PyPI package so others can install via `pip install yourâ€package`. For now, keep it inside `fedops/server/strategies/` and reference it in `config.yaml`.

---

## ğŸ“ Example Commands

1. **Install dependencies** (on each node/pod):

   ```bash
   git clone https://github.com/<yourorg>/Hateful_memes_classification.git
   cd Hateful_memes_classification
   python -m venv venv
   source venv/bin/activate
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

2. **Start FL Server** (port 40025 by default):

   ```bash
   python server_main.py
   ```

3. **Start Client Manager** (port 8004):

   ```bash
   python client_manager_main.py
   ```

4. **Start FL Client** (port 8003):

   ```bash
   python clienta_main.py
   ```

   * For two clients on one machine, simply open two terminals and run both the manager + client pair twice (each gets its own device MAC, so Flower sees 2 distinct clients).

5. **Monitor Logs**

   * Server logs: roundâ€byâ€round validation accuracy.
   * Client logs: â€œtrain\_performanceâ€‰â†’ {train\_loss, train\_accuracy, val\_loss, val\_accuracy}.â€
   * Manager logs: health checks, startâ€training triggers, and final success/fail messages.

---

## âœ… Key Hyperparameters

All of these live in `conf/config.yaml`. Change and reâ€run:

```yaml
random_seed: 42
lr: 0.0001
model_type: 'Pytorch'

# Model architecture:
model:
  _target_: models.HatefulMemesFusionModel
  text_hidden_dim: 768
  image_output_dim: 128
  fusion_output_dim: 256
  output_size: 2

dataset:
  name: 'hateful_memes'
  validation_split: 0.2

# Federated setup:
task_id: 'hatetaskthree'
num_epochs: 1
batch_size: 32
num_rounds: 2
clients_per_round: 2

# Flower Strategy:
server:
  strategy:
    _target_: flwr.server.strategy.FedAvg
    fraction_fit: 1.0      # e.g. 1.0 for all selected clients
    fraction_evaluate: 0.5
    min_fit_clients: ${clients_per_round}
    min_available_clients: ${clients_per_round}
    min_evaluate_clients: ${clients_per_round}
```

* **`num_epochs`**: how many local epochs each client runs per round.
* **`batch_size`**: local miniâ€batch size.
* **`num_rounds`**: total FL rounds.
* **`clients_per_round`**: how many clients to sample per FL round.
* **`fraction_fit`** & **`fraction_evaluate`**: these can be 1.0 (use all), or a fraction if you have >2 clients.

---

## ğŸ” Troubleshooting

* **Missing GPU usage**:

  * Ensure `torch.cuda.is_available()` returns `True`.
  * In `clienta_main.py`, you should see `ğŸ”Œ Using device: cuda:0`.
  * If it prints `cpu`, check that you have a CUDAâ€compatible GPU and proper drivers.

* **â€œâš ï¸ Missing image or URLâ€ logs**:

  * Some Hateful Memes entries may lack a valid image URL. The dataset loader prints a warning and substitutes a zeroâ€tensor. This does not break training; accuracy may degrade slightly.

* **Flower client â€œUNAVAILABLEâ€ / â€œping timeoutâ€**:

  * Check that serverâ€™s gRPC port (40025 by default) is reachable from each client node (firewall, Kubernetes Service config).
  * Verify `server_main.py` actually started without errors.

* **Kubernetes eviction / OOM**:

  * The imageâ€loading in `data_preparation.py` can consume shared memory. Increase `emptyDir.medium: â€œMemoryâ€` or raise `â€“shm-size` if using Docker.
  * In K8s, ensure your Pod spec requests enough RAM (e.g., `resources.requests.memory: â€œ8Giâ€`).

---

## ğŸ“š Citations & References

* **Flower**: â€œFlower: A Friendly Federated Learning Research Frameworkâ€ ([https://flower.dev/](https://flower.dev/))
* **FedOps**: â€œFedOps: Federated Operations Made Easyâ€ ([https://github.com/FedOps/FedOps](https://github.com/FedOps/FedOps))
* **Hateful Memes**: Kiela et al., â€œThe Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memesâ€ (NeurIPS 2020)
* **Hugging Face Datasets**: â€œneuralcatcher/hateful\_memesâ€ on HF.

---

## ğŸ· License

This project is released under the MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

* Built with â¤ï¸ using PyTorch, HuggingFace Transformers, Flower, and FedOps.
* Inspired by â€œNeuralCatcher/Hateful Memesâ€ on Hugging Face.

```
```
