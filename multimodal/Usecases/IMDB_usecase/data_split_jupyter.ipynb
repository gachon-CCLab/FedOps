{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688fe82a",
   "metadata": {},
   "source": [
    "Download dataset via terminal(dont execute this below ltw lines in .ipynb jupyter notebook,instead execute on your terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a090b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "git lfs install\n",
    "git clone https://huggingface.co/datasets/pranavmr/MM-IMDb mmimdb_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "547b456a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Loading local Parquet shards ...\n",
      "Columns: ['image', 'text', 'labels']\n",
      "üñºÔ∏è  Exporting posters and building csv rows ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25959/25959 [00:41<00:00, 618.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wrote CSV: all_in_one_dataset/mmimdb.csv  (rows=25959)\n",
      "‚úÖ Saved posters to: all_in_one_dataset/mmimdb_posters\n",
      "‚úÇÔ∏è  Creating global train/dev/test splits ...\n",
      "üìÇ client_0: train=6541, val=727, test=2596\n",
      "üìÇ client_1: train=1868, val=208, test=2596\n",
      "üìÇ client_2: train=2803, val=312, test=2596\n",
      "üìÇ client_3: train=3737, val=416, test=2596\n",
      "üìÇ client_4: train=3739, val=416, test=2596\n",
      "üñ•Ô∏è  Server eval CSV ‚Üí dataset/server/server_test.csv  (rows=100)\n",
      "üéâ Done. You can now point your loaders to:\n",
      "   POSTERS_DIR = all_in_one_dataset/mmimdb_posters\n",
      "   dataset/client_*/train.csv|val.csv|test.csv  (columns: img_name, text, labels)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# MM-IMDb (HuggingFace pranavmr/MM-IMDb, local Parquet) ‚Üí JPGs + CSV + federated client splits\n",
    "\n",
    "from pathlib import Path\n",
    "import io, json, os, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ------------------------------\n",
    "# Paths (edit if needed)\n",
    "# ------------------------------\n",
    "# where you cloned pranavmr/MM-IMDb\n",
    "HF_LOCAL_DATA = Path(\"/home/ccl/Desktop/akeel_folder/MMFL_Flower/FedOps/silo/examples/torch/new_hateful_memes_classification/new_hateful_memes/FedMAP/IMDB_usecase/mmimdb_hf/data/train-*.parquet\")\n",
    "\n",
    "# your project layout (same style you used before)\n",
    "BASE_DIR = Path(\"./all_in_one_dataset\")\n",
    "POSTERS_DIR = BASE_DIR / \"mmimdb_posters\"\n",
    "CSV_PATH = BASE_DIR / \"mmimdb.csv\"\n",
    "\n",
    "# federated output\n",
    "OUTPUT_DIR = Path(\"dataset\")\n",
    "SERVER_DIR = OUTPUT_DIR / \"server\"\n",
    "\n",
    "# ------------------------------\n",
    "# Split / client config\n",
    "# ------------------------------\n",
    "SEED = 42\n",
    "NUM_CLIENTS = 5\n",
    "PROPORTIONS = [0.35, 0.10, 0.15, 0.2, 0.2]  # must sum to 1.0\n",
    "SERVER_SAMPLE_LIMIT = 100  # rows for server eval csv\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Load local Parquet shards\n",
    "# ------------------------------\n",
    "print(\"üîé Loading local Parquet shards ...\")\n",
    "ds = load_dataset(\"parquet\", data_files={\"train\": str(HF_LOCAL_DATA)}, split=\"train\")\n",
    "print(\"Columns:\", ds.column_names)\n",
    "\n",
    "# sanity: expect image, text, labels (labels is a sequence[str])\n",
    "for col in [\"image\", \"text\", \"labels\"]:\n",
    "    assert col in ds.column_names, f\"Expected column '{col}' not found in dataset.\"\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Export images to JPG + build rows\n",
    "# ------------------------------\n",
    "POSTERS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "rows = []\n",
    "\n",
    "print(\"üñºÔ∏è  Exporting posters and building csv rows ...\")\n",
    "for i, ex in enumerate(tqdm(ds, total=len(ds))):\n",
    "    # image column may already be PIL.Image; otherwise can be dict with \"bytes\" or a path\n",
    "    img = ex[\"image\"]\n",
    "    if isinstance(img, dict) and \"bytes\" in img:\n",
    "        pil = Image.open(io.BytesIO(img[\"bytes\"])).convert(\"RGB\")\n",
    "    elif isinstance(img, Image.Image):\n",
    "        pil = img.convert(\"RGB\")\n",
    "    else:\n",
    "        # path-like\n",
    "        pil = Image.open(img).convert(\"RGB\")\n",
    "\n",
    "    img_name = f\"{i:07d}.jpg\"\n",
    "    pil.save(POSTERS_DIR / img_name, \"JPEG\")\n",
    "\n",
    "    # text\n",
    "    text = ex.get(\"text\", \"\") or \"\"\n",
    "\n",
    "    # labels: list[str] -> pipe-separated\n",
    "    labs = ex.get(\"labels\", [])\n",
    "    if isinstance(labs, str):\n",
    "        label_str = labs\n",
    "    else:\n",
    "        label_str = \"|\".join(sorted({str(x) for x in labs}))\n",
    "\n",
    "    rows.append({\"img_name\": img_name, \"text\": text, \"labels\": label_str})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "CSV_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(CSV_PATH, index=False)\n",
    "print(f\"‚úÖ Wrote CSV: {CSV_PATH}  (rows={len(df)})\")\n",
    "print(f\"‚úÖ Saved posters to: {POSTERS_DIR}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Create global train/dev/test (80/10/10)\n",
    "# ------------------------------\n",
    "print(\"‚úÇÔ∏è  Creating global train/dev/test splits ...\")\n",
    "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "train_all, test_all = train_test_split(df, test_size=0.1, random_state=SEED)\n",
    "train_all, dev_all  = train_test_split(train_all, test_size=0.1111, random_state=SEED)  # ‚âà10% total\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Shard train_all to clients (non-IID sizes by PROPORTIONS)\n",
    "# ------------------------------\n",
    "assert abs(sum(PROPORTIONS) - 1.0) < 1e-6, \"PROPORTIONS must sum to 1.0\"\n",
    "n = len(train_all)\n",
    "cuts = [0]\n",
    "cum = 0\n",
    "for p in PROPORTIONS[:-1]:\n",
    "    cum += int(n * p)\n",
    "    cuts.append(cum)\n",
    "cuts.append(n)\n",
    "\n",
    "client_slices = []\n",
    "for i in range(NUM_CLIENTS):\n",
    "    s, e = cuts[i], cuts[i+1]\n",
    "    client_slices.append(train_all.iloc[s:e].reset_index(drop=True))\n",
    "\n",
    "# optional modality flags (all multimodal here)\n",
    "client_modalities = {i: {\"use_text\": 1, \"use_image\": 1} for i in range(NUM_CLIENTS)}\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Save per-client CSVs + server eval CSV\n",
    "# ------------------------------\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for i, cdf in enumerate(client_slices):\n",
    "    cdir = OUTPUT_DIR / f\"client_{i}\"\n",
    "    cdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # small val split per client\n",
    "    if len(cdf) >= 10:\n",
    "        c_train, c_val = train_test_split(cdf, test_size=0.1, random_state=SEED + i)\n",
    "    else:\n",
    "        c_train, c_val = cdf, cdf.iloc[0:0]\n",
    "\n",
    "    c_train.to_csv(cdir / \"train.csv\", index=False)\n",
    "    c_val.to_csv(cdir / \"val.csv\", index=False)\n",
    "    test_all.to_csv(cdir / \"test.csv\", index=False)\n",
    "\n",
    "    with open(cdir / \"modality.json\", \"w\") as f:\n",
    "        json.dump(client_modalities[i], f)\n",
    "\n",
    "    print(f\"üìÇ client_{i}: train={len(c_train)}, val={len(c_val)}, test={len(test_all)}\")\n",
    "\n",
    "SERVER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "serv_df = dev_all.sample(n=min(SERVER_SAMPLE_LIMIT, len(dev_all)), random_state=SEED).reset_index(drop=True)\n",
    "serv_df.to_csv(SERVER_DIR / \"server_test.csv\", index=False)\n",
    "print(f\"üñ•Ô∏è  Server eval CSV ‚Üí {SERVER_DIR / 'server_test.csv'}  (rows={len(serv_df)})\")\n",
    "\n",
    "print(\"üéâ Done. You can now point your loaders to:\")\n",
    "print(f\"   POSTERS_DIR = {POSTERS_DIR}\")\n",
    "print(f\"   dataset/client_*/train.csv|val.csv|test.csv  (columns: img_name, text, labels)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30fea1",
   "metadata": {},
   "source": [
    "modality hetreogenity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad267576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Loading local Parquet shards ...\n",
      "Columns: ['image', 'text', 'labels']\n",
      "üñºÔ∏è  Exporting posters and building csv rows ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25959/25959 [00:41<00:00, 620.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wrote CSV: all_in_one_dataset/mmimdb.csv  (rows=25959)\n",
      "‚úÖ Saved posters to: all_in_one_dataset/mmimdb_posters\n",
      "‚úÇÔ∏è  Creating global train/dev/test splits ...\n",
      "üìÇ client_0: train=6541, val=727, test=2596 ‚Üí modality={'use_text': 1, 'use_image': 1}\n",
      "üìÇ client_1: train=1868, val=208, test=2596 ‚Üí modality={'use_text': 1, 'use_image': 0}\n",
      "üìÇ client_2: train=2803, val=312, test=2596 ‚Üí modality={'use_text': 0, 'use_image': 1}\n",
      "üìÇ client_3: train=3737, val=416, test=2596 ‚Üí modality={'use_text': 1, 'use_image': 1}\n",
      "üìÇ client_4: train=3739, val=416, test=2596 ‚Üí modality={'use_text': 0, 'use_image': 1}\n",
      "üñ•Ô∏è  Server eval CSV ‚Üí dataset/server/server_test.csv  (rows=100)\n",
      "üéâ Done. Point your loaders to:\n",
      "   POSTERS_DIR = all_in_one_dataset/mmimdb_posters\n",
      "   dataset/client_*/train.csv|val.csv|test.csv  (columns: img_name, text, labels)\n",
      "   Each client has modality.json reflecting its enforced modality.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# MM-IMDb (HuggingFace pranavmr/MM-IMDb, local Parquet) ‚Üí JPGs + CSV + federated client splits\n",
    "# with PER-CLIENT MODALITY CONTROL (text-only / image-only / both)\n",
    "\n",
    "from pathlib import Path\n",
    "import io, json, os, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ------------------------------\n",
    "# Paths (edit if needed)\n",
    "# ------------------------------\n",
    "HF_LOCAL_DATA = Path(\"/home/ccl/Desktop/akeel_folder/MMFL_Flower/FedOps/silo/examples/torch/new_hateful_memes_classification/new_hateful_memes/FedMAP/IMDB_usecase/mmimdb_hf/data/train-*.parquet\")\n",
    "\n",
    "BASE_DIR = Path(\"./all_in_one_dataset\")\n",
    "POSTERS_DIR = BASE_DIR / \"mmimdb_posters\"   # if your loader expects \"img\", change to BASE_DIR / \"img\"\n",
    "CSV_PATH = BASE_DIR / \"mmimdb.csv\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"dataset\")\n",
    "SERVER_DIR = OUTPUT_DIR / \"server\"\n",
    "\n",
    "# ------------------------------\n",
    "# Split / client config\n",
    "# ------------------------------\n",
    "SEED = 42\n",
    "NUM_CLIENTS = 5\n",
    "PROPORTIONS = [0.35, 0.10, 0.15, 0.20, 0.20]  # must sum to 1.0\n",
    "SERVER_SAMPLE_LIMIT = 100\n",
    "\n",
    "# ------------------------------\n",
    "# MODALITY POLICY (edit this)\n",
    "# 1 = keep modality, 0 = remove modality for that client\n",
    "# Examples:\n",
    "#  - text-only: {\"use_text\": 1, \"use_image\": 0}\n",
    "#  - image-only: {\"use_text\": 0, \"use_image\": 1}\n",
    "#  - both: {\"use_text\": 1, \"use_image\": 1}\n",
    "# ------------------------------\n",
    "client_modalities = {\n",
    "    0: {\"use_text\": 1, \"use_image\": 1},  # both\n",
    "    1: {\"use_text\": 1, \"use_image\": 0},  # text-only\n",
    "    2: {\"use_text\": 0, \"use_image\": 1},  # image-only\n",
    "    3: {\"use_text\": 1, \"use_image\": 1},  # both\n",
    "    4: {\"use_text\": 0, \"use_image\": 1},  # image-only\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Load local Parquet shards\n",
    "# ------------------------------\n",
    "print(\"üîé Loading local Parquet shards ...\")\n",
    "ds = load_dataset(\"parquet\", data_files={\"train\": str(HF_LOCAL_DATA)}, split=\"train\")\n",
    "print(\"Columns:\", ds.column_names)\n",
    "\n",
    "for col in [\"image\", \"text\", \"labels\"]:\n",
    "    assert col in ds.column_names, f\"Expected column '{col}' not found in dataset.\"\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Export images to JPG + build rows\n",
    "# ------------------------------\n",
    "POSTERS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "rows = []\n",
    "\n",
    "print(\"üñºÔ∏è  Exporting posters and building csv rows ...\")\n",
    "for i, ex in enumerate(tqdm(ds, total=len(ds))):\n",
    "    img = ex[\"image\"]\n",
    "    if isinstance(img, dict) and \"bytes\" in img:\n",
    "        pil = Image.open(io.BytesIO(img[\"bytes\"])).convert(\"RGB\")\n",
    "    elif isinstance(img, Image.Image):\n",
    "        pil = img.convert(\"RGB\")\n",
    "    else:\n",
    "        pil = Image.open(img).convert(\"RGB\")\n",
    "\n",
    "    img_name = f\"{i:07d}.jpg\"\n",
    "    pil.save(POSTERS_DIR / img_name, \"JPEG\")\n",
    "\n",
    "    text = ex.get(\"text\", \"\") or \"\"\n",
    "\n",
    "    labs = ex.get(\"labels\", [])\n",
    "    if isinstance(labs, str):\n",
    "        label_str = labs\n",
    "    else:\n",
    "        label_str = \"|\".join(sorted({str(x) for x in labs}))\n",
    "\n",
    "    rows.append({\"img_name\": img_name, \"text\": text, \"labels\": label_str})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "CSV_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(CSV_PATH, index=False)\n",
    "print(f\"‚úÖ Wrote CSV: {CSV_PATH}  (rows={len(df)})\")\n",
    "print(f\"‚úÖ Saved posters to: {POSTERS_DIR}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Create global train/dev/test (80/10/10)\n",
    "# ------------------------------\n",
    "print(\"‚úÇÔ∏è  Creating global train/dev/test splits ...\")\n",
    "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "train_all, test_all = train_test_split(df, test_size=0.1, random_state=SEED)\n",
    "train_all, dev_all  = train_test_split(train_all, test_size=0.1111, random_state=SEED)  # ‚âà10% total\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Shard train_all to clients (non-IID sizes by PROPORTIONS)\n",
    "# ------------------------------\n",
    "assert abs(sum(PROPORTIONS) - 1.0) < 1e-6, \"PROPORTIONS must sum to 1.0\"\n",
    "n = len(train_all)\n",
    "cuts = [0]\n",
    "cum = 0\n",
    "for p in PROPORTIONS[:-1]:\n",
    "    cum += int(n * p)\n",
    "    cuts.append(cum)\n",
    "cuts.append(n)\n",
    "\n",
    "client_slices = []\n",
    "for i in range(NUM_CLIENTS):\n",
    "    s, e = cuts[i], cuts[i+1]\n",
    "    client_slices.append(train_all.iloc[s:e].reset_index(drop=True))\n",
    "\n",
    "# ------------------------------\n",
    "# Helper: apply per-client modality mask BEFORE saving CSVs\n",
    "# ------------------------------\n",
    "# Create one reusable blank image for image-masked clients\n",
    "blank_img_path = POSTERS_DIR / \"__BLANK__.jpg\"\n",
    "if not blank_img_path.exists():\n",
    "    Image.new(\"RGB\", (224, 224), (0, 0, 0)).save(blank_img_path, \"JPEG\")\n",
    "\n",
    "def apply_modality_mask(df_in: pd.DataFrame, m: dict) -> pd.DataFrame:\n",
    "    \"\"\"Strip text and/or replace image with a blank file as per client policy.\"\"\"\n",
    "    df = df_in.copy()\n",
    "    if m.get(\"use_text\", 1) == 0:\n",
    "        df[\"text\"] = \"\"\n",
    "    if m.get(\"use_image\", 1) == 0:\n",
    "        df[\"img_name\"] = \"__BLANK__.jpg\"\n",
    "    return df\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Save per-client CSVs + server eval CSV (with enforced modality)\n",
    "# ------------------------------\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for i, cdf in enumerate(client_slices):\n",
    "    cdir = OUTPUT_DIR / f\"client_{i}\"\n",
    "    cdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if len(cdf) >= 10:\n",
    "        c_train, c_val = train_test_split(cdf, test_size=0.1, random_state=SEED + i)\n",
    "    else:\n",
    "        c_train, c_val = cdf, cdf.iloc[0:0]\n",
    "\n",
    "    m = client_modalities.get(i, {\"use_text\": 1, \"use_image\": 1})\n",
    "    c_train_masked = apply_modality_mask(c_train, m)\n",
    "    c_val_masked   = apply_modality_mask(c_val,   m)\n",
    "    test_masked    = apply_modality_mask(test_all, m)\n",
    "\n",
    "    c_train_masked.to_csv(cdir / \"train.csv\", index=False)\n",
    "    c_val_masked.to_csv(cdir / \"val.csv\", index=False)\n",
    "    test_masked.to_csv(cdir / \"test.csv\", index=False)\n",
    "\n",
    "    with open(cdir / \"modality.json\", \"w\") as f:\n",
    "        json.dump(m, f)\n",
    "\n",
    "    print(f\"üìÇ client_{i}: train={len(c_train_masked)}, val={len(c_val_masked)}, test={len(test_masked)} ‚Üí modality={m}\")\n",
    "\n",
    "SERVER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "serv_df = dev_all.sample(n=min(SERVER_SAMPLE_LIMIT, len(dev_all)), random_state=SEED).reset_index(drop=True)\n",
    "serv_df.to_csv(SERVER_DIR / \"server_test.csv\", index=False)\n",
    "print(f\"üñ•Ô∏è  Server eval CSV ‚Üí {SERVER_DIR / 'server_test.csv'}  (rows={len(serv_df)})\")\n",
    "\n",
    "print(\"üéâ Done. Point your loaders to:\")\n",
    "print(f\"   POSTERS_DIR = {POSTERS_DIR}\")\n",
    "print(f\"   dataset/client_*/train.csv|val.csv|test.csv  (columns: img_name, text, labels)\")\n",
    "print(\"   Each client has modality.json reflecting its enforced modality.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027e7e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels = 23\n"
     ]
    }
   ],
   "source": [
    "# run once (e.g., python - <<'PY' ... PY)\n",
    "import json, pandas as pd, os\n",
    "CSV = \"all_in_one_dataset/mmimdb.csv\"\n",
    "labs = set()\n",
    "for s in pd.read_csv(CSV, usecols=[\"labels\"])[\"labels\"].astype(str):\n",
    "    labs.update([t for t in s.split(\"|\") if t.strip() != \"\"])\n",
    "label_list = sorted(labs)\n",
    "os.makedirs(\"all_in_one_dataset\", exist_ok=True)\n",
    "json.dump(label_list, open(\"all_in_one_dataset/labels.json\",\"w\"))\n",
    "print(\"num_labels =\", len(label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316cfbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.11.11)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: certifi in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2024.12.14)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ccl/anaconda3/envs/akeel_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "Using cached xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: xxhash, sniffio, pyarrow, h11, dill, multiprocess, httpcore, anyio, httpx, datasets\n",
      "Successfully installed anyio-4.11.0 datasets-4.3.0 dill-0.4.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 multiprocess-0.70.16 pyarrow-22.0.0 sniffio-1.3.1 xxhash-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc452ea3",
   "metadata": {},
   "source": [
    "server data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90749812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 rows in dataset/server/server_test.csv\n",
      "‚úÖ Images placed in: server_data/mmimdb_posters\n",
      "‚úÖ CSV copied to:    server_data/server_test.csv\n",
      "   Copied/Symlinked: 100\n",
      "üì¶ Creating zip: server_data.zip\n",
      "üéâ Done: server_data.zip\n"
     ]
    }
   ],
   "source": [
    "# JUPYTER CELL ‚Äî build a minimal server package from your existing server_test.csv\n",
    "\n",
    "import os, csv, shutil, zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# --- paths (edit if needed) ---\n",
    "SERVER_CSV = Path(\"dataset/server/server_test.csv\")\n",
    "SRC_IMG_DIR = Path(\"all_in_one_dataset/mmimdb_posters\")  # where your exporter saved all posters\n",
    "DEST_ROOT   = Path(\"server_data\")                        # new minimal package root\n",
    "DEST_IMG_DIR = DEST_ROOT / \"mmimdb_posters\"\n",
    "MAKE_ZIP = True                                          # False if you don't want a zip\n",
    "MODE = \"copy\"                                            # \"copy\" or \"symlink\"\n",
    "\n",
    "# --- helpers ---\n",
    "def ensure_dirs():\n",
    "    DEST_IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    DEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def gather_img_names(csv_path: Path):\n",
    "    img_names = []\n",
    "    with csv_path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if \"img_name\" not in reader.fieldnames:\n",
    "            raise ValueError(f\"'img_name' column not found in {csv_path} (columns={reader.fieldnames})\")\n",
    "        for row in reader:\n",
    "            name = row[\"img_name\"].strip()\n",
    "            if name:\n",
    "                img_names.append(name)\n",
    "    return img_names\n",
    "\n",
    "def copy_or_link(src: Path, dst: Path, mode: str):\n",
    "    if mode == \"symlink\":\n",
    "        try:\n",
    "            if dst.exists():\n",
    "                dst.unlink()\n",
    "            os.symlink(os.path.abspath(src), os.path.abspath(dst))\n",
    "        except Exception:\n",
    "            shutil.copy2(src, dst)  # fallback\n",
    "    else:\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "# --- main ---\n",
    "assert SERVER_CSV.exists(), f\"CSV not found: {SERVER_CSV}\"\n",
    "assert SRC_IMG_DIR.is_dir(), f\"SRC_IMG_DIR not found: {SRC_IMG_DIR}\"\n",
    "\n",
    "ensure_dirs()\n",
    "img_names = gather_img_names(SERVER_CSV)\n",
    "\n",
    "print(f\"Found {len(img_names)} rows in {SERVER_CSV}\")\n",
    "copied = 0\n",
    "missing = []\n",
    "\n",
    "for name in img_names:\n",
    "    src = SRC_IMG_DIR / name\n",
    "    dst = DEST_IMG_DIR / name\n",
    "    if src.exists():\n",
    "        copy_or_link(src, dst, MODE)\n",
    "        copied += 1\n",
    "    else:\n",
    "        missing.append(name)\n",
    "\n",
    "# copy the csv itself\n",
    "shutil.copy2(SERVER_CSV, DEST_ROOT / \"server_test.csv\")\n",
    "\n",
    "print(f\"‚úÖ Images placed in: {DEST_IMG_DIR}\")\n",
    "print(f\"‚úÖ CSV copied to:    {DEST_ROOT / 'server_test.csv'}\")\n",
    "print(f\"   Copied/Symlinked: {copied}\")\n",
    "if missing:\n",
    "    print(f\"   Missing ({len(missing)}): e.g. {missing[:5]} ...\")\n",
    "\n",
    "if MAKE_ZIP:\n",
    "    zip_path = DEST_ROOT.with_suffix(\".zip\")  # 'server_data.zip'\n",
    "    print(f\"üì¶ Creating zip: {zip_path}\")\n",
    "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.write(DEST_ROOT / \"server_test.csv\", arcname=\"server_test.csv\")\n",
    "        for p in DEST_IMG_DIR.iterdir():\n",
    "            if p.is_file():\n",
    "                zf.write(p, arcname=f\"img/{p.name}\")\n",
    "    print(f\"üéâ Done: {zip_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "akeel_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
