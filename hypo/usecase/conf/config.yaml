# Common

#conf/config.yaml
random_seed: 42

learning_rate: 0.001 # Input model's learning rate
hyperparams: 
- [0.001, 128]
- [0.005, 64]
- [0.01, 32]

model_type: 'Pytorch' # This value should be maintained
model:
  _target_: models.MNISTClassifier # Input your custom model
  output_size: 10 # Input your model's output size (only classification)

dataset:
    name: 'MNIST' # Input your data name
    validation_split: 0.2 # Ratio of dividing train data by validation


# client
task_id: 'clustert' # Input your Task Name that you register in FedOps Website

wandb: 
  use: true # Whether to use wandb
  key: '38f6cf3c2c37660d42ebfe8ab434b72d34be3b31' # Input your wandb api key
  account: 'rirakang@gachon.ac.kr' # Input your wandb account
  project: '${dataset.name}_${task_id}'


# server
num_epochs: 1 # number of local epochs
batch_size: 128
num_rounds: 2 # Number of rounds to perform
clients_per_round: 3 # Number of clients participating in the round

server:
  strategy:
    _target_: fedops.server.strategy_cluster_optuna.ClusterOptunaFedAvg
    fraction_fit: 0.00001
    fraction_evaluate: 0.000001
    min_fit_clients: ${clients_per_round}
    min_available_clients: ${clients_per_round}
    min_evaluate_clients: ${clients_per_round}

    # === 클러스터링/HPO 옵션 ===
    warmup_rounds: 1
    recluster_every: 1
    eps: 0.2
    min_samples: 2
    objective: "maximize_f1"        # "maximize_acc" / "minimize_loss" 도 가능
    search_lr_log: [-5.0, -2.0]     # 1e-5 ~ 1e-2
    search_bs_exp: [3, 7]           # 2^3~2^7 = 8~128
    search_local_epochs: [1, 3]
    # seed_points: [[0.001, 128], [0.005, 64], [0.01, 32]]  # 선택: warm start


